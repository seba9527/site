<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Machine_learning 1  -  My way, though far away</title>
<meta name="description" content="开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。机器学习定义这里给出了2种机器学习的定义：Arthur Samuel described it as: &quot;the field of study that gives computers the ability to learn without being explicitly programmed.&quot; This is an older, informal definition.Tom Mitchell provides a more modern definition: &quot;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&quot;Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game.大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。监督学习在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。回归问题回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。分类问题那么分类问题就是对离散的问题来说的，对离散的问题进行预测。这里给2个例子，区分一下。**Example 1:**Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.**Example 2**:(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.无监督学习不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。聚类算法所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。鸡尾酒宴席问题（分散算法）同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。下面给出这两个问题的描述。**Example:**Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.Non-clustering: The &quot;Cocktail Party Algorithm&quot;, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a)模型展示监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。代价函数先给出代价函数的定义。这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。可视化代价函数为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。梯度下降算法 (Gradient Descent)梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。关于参数a，需要指出的有2点。  1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。  2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。矩阵和向量这里都是现代里面的知识了，我就不多说了，基本都会。Week 1 ends here.">


  <meta name="author" content="zjgcjy">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="My way, though far away">
<meta property="og:title" content="Machine_learning 1">
<meta property="og:url" content="https://zjgcjy.github.io/posts/2018-12/Machine_Learning-1.html">


  <meta property="og:description" content="开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。机器学习定义这里给出了2种机器学习的定义：Arthur Samuel described it as: &quot;the field of study that gives computers the ability to learn without being explicitly programmed.&quot; This is an older, informal definition.Tom Mitchell provides a more modern definition: &quot;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&quot;Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game.大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。监督学习在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。回归问题回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。分类问题那么分类问题就是对离散的问题来说的，对离散的问题进行预测。这里给2个例子，区分一下。**Example 1:**Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.**Example 2**:(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.无监督学习不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。聚类算法所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。鸡尾酒宴席问题（分散算法）同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。下面给出这两个问题的描述。**Example:**Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.Non-clustering: The &quot;Cocktail Party Algorithm&quot;, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a)模型展示监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。代价函数先给出代价函数的定义。这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。可视化代价函数为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。梯度下降算法 (Gradient Descent)梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。关于参数a，需要指出的有2点。  1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。  2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。矩阵和向量这里都是现代里面的知识了，我就不多说了，基本都会。Week 1 ends here.">



  <meta property="og:image" content="https://zjgcjy.github.io/assets/images/bio-photo.jpg">



  <meta name="twitter:site" content="@zjgcjy">
  <meta name="twitter:title" content="Machine_learning 1">
  <meta name="twitter:description" content="开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。机器学习定义这里给出了2种机器学习的定义：Arthur Samuel described it as: &quot;the field of study that gives computers the ability to learn without being explicitly programmed.&quot; This is an older, informal definition.Tom Mitchell provides a more modern definition: &quot;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&quot;Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game.大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。监督学习在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。回归问题回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。分类问题那么分类问题就是对离散的问题来说的，对离散的问题进行预测。这里给2个例子，区分一下。**Example 1:**Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.**Example 2**:(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.无监督学习不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。聚类算法所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。鸡尾酒宴席问题（分散算法）同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。下面给出这两个问题的描述。**Example:**Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.Non-clustering: The &quot;Cocktail Party Algorithm&quot;, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a)模型展示监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。代价函数先给出代价函数的定义。这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。可视化代价函数为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。梯度下降算法 (Gradient Descent)梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。关于参数a，需要指出的有2点。  1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。  2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。矩阵和向量这里都是现代里面的知识了，我就不多说了，基本都会。Week 1 ends here.">
  <meta name="twitter:url" content="https://zjgcjy.github.io/posts/2018-12/Machine_Learning-1.html">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://zjgcjy.github.io/assets/images/bio-photo.jpg">
    
  

  



  <meta property="article:published_time" content="2018-12-11T20:10:00+08:00">





  

  


<link rel="canonical" href="https://zjgcjy.github.io/posts/2018-12/Machine_Learning-1.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Blogger",
      "url": "https://zjgcjy.github.io/",
      "sameAs": ["https://twitter.com/","https://github.com/"]
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="My way, though far away Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/png" href="/assets/images/hacker_cyber_crime-512.png">
<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/hacker-inside.png" alt=""></a>
        
        <a class="site-title" href="/">
          ZJGCJY
          <span class="site-subtitle">My way, though far away</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/year-archive/" >Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/" >Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/" >Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/collection-archive/" >Collections</a>
            </li><li class="masthead__menu-item">
              <a href="/about.html" >About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/me.jpg" alt="zjgcjy" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">zjgcjy</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>IIE UCAS, CTFer, Binarian</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fa fa-map-marker" aria-hidden="true"></i> <span itemprop="name">BeiJing China</span>
        </li>
      

      
        
          
            <li><a href="https://zjgcjy.github.io" rel="nofollow noopener noreferrer"><i class="fa fa-link" aria-hidden="true"></i> Website</a></li>
          
        
          
            <li><a href="mailto:zjgcjy@gmail.com" rel="nofollow noopener noreferrer"><i class="fa fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://github.com/zjgcjy" rel="nofollow noopener noreferrer"><i class="fa fa-github-square" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://twitter.com/zjgcjy" rel="nofollow noopener noreferrer"><i class="fa fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Machine_learning 1">
    <meta itemprop="description" content="开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。机器学习定义这里给出了2种机器学习的定义：Arthur Samuel described it as: &quot;the field of study that gives computers the ability to learn without being explicitly programmed.&quot; This is an older, informal definition.Tom Mitchell provides a more modern definition: &quot;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&quot;Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game.大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。监督学习在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。回归问题回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。分类问题那么分类问题就是对离散的问题来说的，对离散的问题进行预测。这里给2个例子，区分一下。**Example 1:**Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.**Example 2**:(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.无监督学习不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。聚类算法所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。鸡尾酒宴席问题（分散算法）同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。下面给出这两个问题的描述。**Example:**Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.Non-clustering: The &quot;Cocktail Party Algorithm&quot;, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a)模型展示监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。代价函数先给出代价函数的定义。这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。可视化代价函数为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。梯度下降算法 (Gradient Descent)梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。关于参数a，需要指出的有2点。  1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。  2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。矩阵和向量这里都是现代里面的知识了，我就不多说了，基本都会。Week 1 ends here.">
    <meta itemprop="datePublished" content="December 11, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Machine_learning 1
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#机器学习定义">机器学习定义</a></li>
  <li><a href="#监督学习">监督学习</a>
    <ul>
      <li><a href="#回归问题">回归问题</a></li>
      <li><a href="#分类问题">分类问题</a></li>
    </ul>
  </li>
  <li><a href="#无监督学习">无监督学习</a>
    <ul>
      <li><a href="#聚类算法">聚类算法</a></li>
      <li><a href="#鸡尾酒宴席问题分散算法">鸡尾酒宴席问题（分散算法）</a></li>
    </ul>
  </li>
  <li><a href="#模型展示">模型展示</a></li>
  <li><a href="#代价函数">代价函数</a></li>
  <li><a href="#可视化代价函数">可视化代价函数</a></li>
  <li><a href="#梯度下降算法-gradient-descent">梯度下降算法 (Gradient Descent)</a></li>
  <li><a href="#应用于线性回归的梯度下降算法gradient-descent-for-linear-regression">应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)</a></li>
  <li><a href="#矩阵和向量">矩阵和向量</a></li>
</ul>
            </nav>
          </aside>
        
        <p>开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。</p>

<hr />

<h1 id="机器学习定义">机器学习定义</h1>

<p>这里给出了2种机器学习的定义：</p>

<pre><code class="language-language">Arthur Samuel described it as: "the field of study that gives computers the ability to learn without being explicitly programmed." This is an older, informal definition.

Tom Mitchell provides a more modern definition: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."

Example: playing checkers.
E = the experience of playing many games of checkers
T = the task of playing checkers.
P = the probability that the program will win the next game.
</code></pre>

<p>大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。</p>

<h1 id="监督学习">监督学习</h1>

<p>在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。</p>

<h2 id="回归问题">回归问题</h2>

<p>回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-08-58.png" alt="回归问题" /></p>

<h2 id="分类问题">分类问题</h2>

<p>那么分类问题就是对离散的问题来说的，对离散的问题进行预测。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-13-14.png" alt="分类问题1" /></p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-15-39.png" alt="分类问题2" /></p>

<p>这里给2个例子，区分一下。</p>

<pre><code class="language-language">**Example 1:**

Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.

We could turn this example into a classification problem by instead making our output about whether the house "sells for more or less than the asking price." Here we are classifying the houses based on price into two discrete categories.

**Example 2**:

(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture

(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.
</code></pre>

<h1 id="无监督学习">无监督学习</h1>

<p>不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-22-52.png" alt="监督学习" /></p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-25-32.png" alt="无监督学习" /></p>

<h2 id="聚类算法">聚类算法</h2>

<p>所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-31-15.png" alt="聚类算法" /></p>

<h2 id="鸡尾酒宴席问题分散算法">鸡尾酒宴席问题（分散算法）</h2>

<p>同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-11_21-44-53.png" alt="鸡尾酒宴席问题" /></p>

<p>下面给出这两个问题的描述。</p>

<pre><code class="language-language">**Example:**

Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.

Non-clustering: The "Cocktail Party Algorithm", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a)
</code></pre>

<h1 id="模型展示">模型展示</h1>

<p>监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。
我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_13-08-33.png" alt="线性回归模型" /></p>

<p>线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。</p>

<h1 id="代价函数">代价函数</h1>

<p>先给出代价函数的定义。</p>

<script type="math/tex; mode=display">J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2</script>

<p>这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_13-51-55.png" alt="估计函数和代价函数" /></p>

<p>代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_13-29-12.png" alt="代价函数定义" /></p>

<h1 id="可视化代价函数">可视化代价函数</h1>

<p>为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_13-41-18.png" alt="代价函数" /></p>

<p>注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。</p>

<p>但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。</p>

<p><img src="/img/2018-12/4.png" alt="二维代价函数" /></p>

<p><img src="/img/2018-12/5.png" alt="二维代价函数" /></p>

<h1 id="梯度下降算法-gradient-descent">梯度下降算法 (Gradient Descent)</h1>

<p>梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。</p>

<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script>

<p><img src="/img/2018-12/Snipaste_2018-12-12_15-24-50.png" alt="梯度下降算法" /></p>

<p>梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。</p>

<p><img src="/img/2018-12/6.png" alt="梯度下降算法" /></p>

<p>关于参数a，需要指出的有2点。</p>

<ul>
  <li>1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。</li>
</ul>

<p><img src="/img/2018-12/Snipaste_2018-12-12_15-47-01.png" alt="梯度下降算法" /></p>

<ul>
  <li>2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。</li>
</ul>

<p><img src="/img/2018-12/Snipaste_2018-12-12_15-50-37.png" alt="梯度下降算法" /></p>

<h1 id="应用于线性回归的梯度下降算法gradient-descent-for-linear-regression">应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)</h1>

<p>那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_19-31-33.png" alt="线性回归和梯度下降" /></p>

<p>那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。</p>

<p><img src="/img/2018-12/Snipaste_2018-12-12_16-14-58.png" alt="梯度下降" /></p>

<p>这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。</p>

<h1 id="矩阵和向量">矩阵和向量</h1>

<p>这里都是现代里面的知识了，我就不多说了，基本都会。</p>

<p><strong>Week 1 ends here.</strong></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#ml" class="page__taxonomy-item" rel="tag">ml</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fa fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-12-11T20:10:00+08:00">December 11, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=zjgcjy&text=Machine_learning+1%20https%3A%2F%2Fzjgcjy.github.io%2Fposts%2F2018-12%2FMachine_Learning-1.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fa fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fzjgcjy.github.io%2Fposts%2F2018-12%2FMachine_Learning-1.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fa fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fzjgcjy.github.io%2Fposts%2F2018-12%2FMachine_Learning-1.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fa fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/posts/2018-12/Git_Learning-3.html" class="pagination--pager" title="Git_learning 3
">Previous</a>
    
    
      <a href="/posts/2018-12/Machine_Learning-2.html" class="pagination--pager" title="Machine_learning 2
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Leave a comment</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2020-01/scoop_usage.html" rel="permalink">usage of scoop
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  重装系统，开始使用scoop

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2020-01/daily_notes.html" rel="permalink">daily notes of 2020-01
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">1月1日
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2019-12/diary_notes.html" rel="permalink">Diary_notes
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  最近很浮躁，日记好久没写了，书也没看，论文也没看，运动也是断断续续。


12月5日

打算重新开blog，找了很久，从stackedit到blogger，从hexo到wordpress再到jekyll。因为图片的缘故，还在vps上搭了rclone来同步onedrive当图床用，后来又用git repo来当...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/posts/2019-11/Dirary_November.html" rel="permalink">Dirary_november
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  每日所作所得




11月1日
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/zjgcjy" rel="nofollow noopener noreferrer"><i class="fa fa-github-square" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/zjgcjy" rel="nofollow noopener noreferrer"><i class="fa fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Blogger. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/1d25d6ba93.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <div id="disqus_thread"></div>
  <script>
    /**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
    */
    /*
    var disqus_config = function () {
      this.page.url = "https://zjgcjy.github.io/posts/2018-12/Machine_Learning-1.html";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/posts/2018-12/Machine_Learning-1"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://zjgcjy-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
