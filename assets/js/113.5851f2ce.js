(window.webpackJsonp=window.webpackJsonp||[]).push([[113],{546:function(t,s,a){"use strict";a.r(s);var e=a(14),r=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。")]),t._v(" "),a("hr"),t._v(" "),a("h2",{attrs:{id:"机器学习定义"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习定义"}},[t._v("#")]),t._v(" 机器学习定义")]),t._v(" "),a("p",[t._v("这里给出了2种机器学习的定义：")]),t._v(" "),a("div",{staticClass:"language-sh line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[t._v("Arthur Samuel described it as: "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"the field of study that gives computers the ability to learn without being explicitly programmed."')]),t._v(" This is an older, informal definition.\n\nTom Mitchell provides a "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("more")]),t._v(" modern definition: "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."')]),t._v("\n\nExample: playing checkers.\nE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" the experience of playing many games of checkers\nT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" the task of playing checkers.\nP "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" the probability that the program will win the next game.\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("p",[t._v("大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。")]),t._v(" "),a("h2",{attrs:{id:"监督学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#监督学习"}},[t._v("#")]),t._v(" 监督学习")]),t._v(" "),a("p",[t._v("在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。")]),t._v(" "),a("h3",{attrs:{id:"回归问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#回归问题"}},[t._v("#")]),t._v(" 回归问题")]),t._v(" "),a("p",[t._v("回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-08-58.png",alt:"回归问题"}})]),t._v(" "),a("h3",{attrs:{id:"分类问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分类问题"}},[t._v("#")]),t._v(" 分类问题")]),t._v(" "),a("p",[t._v("那么分类问题就是对离散的问题来说的，对离散的问题进行预测。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-13-14.png",alt:"分类问题1"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-15-39.png",alt:"分类问题2"}})]),t._v(" "),a("p",[t._v("这里给2个例子，区分一下。")]),t._v(" "),a("div",{staticClass:"language-sh line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[t._v("**Example "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(":**\n\nGiven data about the size of houses on the real estate market, try to predict their price. Price as a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("function")]),t._v(" of size is a continuous output, so this is a regression problem.\n\nWe could turn this example into a classification problem by instead making our output about whether the house "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sells for more or less than the asking price."')]),t._v(" Here we are classifying the houses based on price into two discrete categories.\n\n**Example "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("**:\n\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" Regression - Given a picture of a person, we have to predict their age on the basis of the given picture\n\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("h2",{attrs:{id:"无监督学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#无监督学习"}},[t._v("#")]),t._v(" 无监督学习")]),t._v(" "),a("p",[t._v("不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-22-52.png",alt:"监督学习"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-25-32.png",alt:"无监督学习"}})]),t._v(" "),a("h3",{attrs:{id:"聚类算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#聚类算法"}},[t._v("#")]),t._v(" 聚类算法")]),t._v(" "),a("p",[t._v("所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-31-15.png",alt:"聚类算法"}})]),t._v(" "),a("h3",{attrs:{id:"鸡尾酒宴席问题（分散算法）"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#鸡尾酒宴席问题（分散算法）"}},[t._v("#")]),t._v(" 鸡尾酒宴席问题（分散算法）")]),t._v(" "),a("p",[t._v("同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-11_21-44-53.png",alt:"鸡尾酒宴席问题"}})]),t._v(" "),a("p",[t._v("下面给出这两个问题的描述。")]),t._v(" "),a("div",{staticClass:"language-sh line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sh"}},[a("code",[t._v("**Example:**\n\nClustering: Take a collection of "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1,000")]),t._v(",000 different genes, and "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("find")]),t._v(" a way to automatically group these genes into "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("groups")]),t._v(" that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n\nNon-clustering: The "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Cocktail Party Algorithm"')]),t._v(", allows you to "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("find")]),t._v(" structure "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" a chaotic environment. "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i.e. identifying individual voices and music from a mesh of sounds at a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h2",{attrs:{id:"模型展示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型展示"}},[t._v("#")]),t._v(" 模型展示")]),t._v(" "),a("p",[t._v("监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。\n我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_13-08-33.png",alt:"线性回归模型"}})]),t._v(" "),a("p",[t._v("线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。")]),t._v(" "),a("h2",{attrs:{id:"代价函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#代价函数"}},[t._v("#")]),t._v(" 代价函数")]),t._v(" "),a("p",[t._v("先给出代价函数的定义。")]),t._v(" "),a("p",[t._v("$$\nJ(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum "),a("em",[t._v("{i=1}^m \\left ( \\hat{y}")]),t._v("{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum "),a("em",[t._v("{i=1}^m \\left (h")]),t._v("\\theta (x_{i}) - y_{i} \\right)^2\n$$")]),t._v(" "),a("p",[t._v("这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_13-51-55.png",alt:"估计函数和代价函数"}})]),t._v(" "),a("p",[t._v("代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_13-29-12.png",alt:"代价函数定义"}})]),t._v(" "),a("h2",{attrs:{id:"可视化代价函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#可视化代价函数"}},[t._v("#")]),t._v(" 可视化代价函数")]),t._v(" "),a("p",[t._v("为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_13-41-18.png",alt:"代价函数"}})]),t._v(" "),a("p",[t._v("注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。")]),t._v(" "),a("p",[t._v("但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/4.png",alt:"二维代价函数"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/5.png",alt:"二维代价函数"}})]),t._v(" "),a("h2",{attrs:{id:"梯度下降算法-gradient-descent"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降算法-gradient-descent"}},[t._v("#")]),t._v(" 梯度下降算法 (Gradient Descent)")]),t._v(" "),a("p",[t._v("梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。")]),t._v(" "),a("p",[t._v("$$\n\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\n$$")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_15-24-50.png",alt:"梯度下降算法"}})]),t._v(" "),a("p",[t._v("梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/6.png",alt:"梯度下降算法"}})]),t._v(" "),a("p",[t._v("关于参数a，需要指出的有2点。")]),t._v(" "),a("ul",[a("li",[t._v("1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_15-47-01.png",alt:"梯度下降算法"}})]),t._v(" "),a("ul",[a("li",[t._v("2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_15-50-37.png",alt:"梯度下降算法"}})]),t._v(" "),a("h1",{attrs:{id:"应用于线性回归的梯度下降算法-gradient-descent-for-linear-regression"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用于线性回归的梯度下降算法-gradient-descent-for-linear-regression"}},[t._v("#")]),t._v(" 应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)")]),t._v(" "),a("p",[t._v("那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_19-31-33.png",alt:"线性回归和梯度下降"}})]),t._v(" "),a("p",[t._v("那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。")]),t._v(" "),a("p",[a("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-12_16-14-58.png",alt:"梯度下降"}})]),t._v(" "),a("p",[t._v("这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。")]),t._v(" "),a("h2",{attrs:{id:"矩阵和向量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#矩阵和向量"}},[t._v("#")]),t._v(" 矩阵和向量")]),t._v(" "),a("p",[t._v("这里都是现代里面的知识了，我就不多说了，基本都会。")]),t._v(" "),a("p",[a("strong",[t._v("Week 1 ends here.")])])])}),[],!1,null,null,null);s.default=r.exports}}]);