(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{551:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-08-58.1a6a0329.png"},552:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-13-14.beaf5027.png"},553:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-15-39.f5538e2e.png"},554:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-22-52.7c1af3d0.png"},555:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-25-32.390e2c2f.png"},556:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-31-15.21c0eb2e.png"},557:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-11_21-44-53.1c8c1a47.png"},558:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_13-08-33.bb598ef5.png"},559:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_13-51-55.ee3fa419.png"},560:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_13-29-12.5204cd7d.png"},561:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_13-41-18.835f9c29.png"},562:function(s,t,a){s.exports=a.p+"assets/img/4.bc641973.png"},563:function(s,t,a){s.exports=a.p+"assets/img/5.3ace77dd.png"},564:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_15-24-50.fbc12368.png"},565:function(s,t,a){s.exports=a.p+"assets/img/6.d46402f1.png"},566:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_15-47-01.4de836f6.png"},567:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_15-50-37.a959af7f.png"},568:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_19-31-33.2536b69c.png"},569:function(s,t,a){s.exports=a.p+"assets/img/Snipaste_2018-12-12_16-14-58.f0008476.png"},847:function(s,t,a){"use strict";a.r(t);var e=a(6),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,e=s._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("p",[s._v("开始学习机器学习，记录下第一周的笔记，希望自己能坚持吧。")]),s._v(" "),e("hr"),s._v(" "),e("h2",{attrs:{id:"机器学习定义"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#机器学习定义"}},[s._v("#")]),s._v(" 机器学习定义")]),s._v(" "),e("p",[s._v("这里给出了2种机器学习的定义：")]),s._v(" "),e("div",{staticClass:"language-sh line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[s._v("Arthur Samuel described it as: "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"the field of study that gives computers the ability to learn without being explicitly programmed."')]),s._v(" This is an older, informal definition.\n\nTom Mitchell provides a "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("more")]),s._v(" modern definition: "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."')]),s._v("\n\nExample: playing checkers.\nE "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" the experience of playing many games of checkers\nT "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" the task of playing checkers.\nP "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" the probability that the program will win the next game.\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])]),e("p",[s._v("大体上，任何机器学习问题都额能分成2种类型，一种是是监督学习，一种是非监督学习。")]),s._v(" "),e("h2",{attrs:{id:"监督学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#监督学习"}},[s._v("#")]),s._v(" 监督学习")]),s._v(" "),e("p",[s._v("在监督学习中，对于数据中的每个数据，都有相应的正确答案（训练集），而监督学习就是基于这些数据进行预测。那么这里介绍了2中监督学习的方法，分别是回归问题和分类问题。")]),s._v(" "),e("h3",{attrs:{id:"回归问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#回归问题"}},[s._v("#")]),s._v(" 回归问题")]),s._v(" "),e("p",[s._v("回归问题是对于连续的问题来说的，基于训练集进行预测，训练集都是有正确答案的。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(551),alt:"回归问题"}})]),s._v(" "),e("h3",{attrs:{id:"分类问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分类问题"}},[s._v("#")]),s._v(" 分类问题")]),s._v(" "),e("p",[s._v("那么分类问题就是对离散的问题来说的，对离散的问题进行预测。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(552),alt:"分类问题1"}})]),s._v(" "),e("p",[e("img",{attrs:{src:a(553),alt:"分类问题2"}})]),s._v(" "),e("p",[s._v("这里给2个例子，区分一下。")]),s._v(" "),e("div",{staticClass:"language-sh line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[s._v("**Example "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(":**\n\nGiven data about the size of houses on the real estate market, try to predict their price. Price as a "),e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("function")]),s._v(" of size is a continuous output, so this is a regression problem.\n\nWe could turn this example into a classification problem by instead making our output about whether the house "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"sells for more or less than the asking price."')]),s._v(" Here we are classifying the houses based on price into two discrete categories.\n\n**Example "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("**:\n\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" Regression - Given a picture of a person, we have to predict their age on the basis of the given picture\n\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br")])]),e("h2",{attrs:{id:"无监督学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#无监督学习"}},[s._v("#")]),s._v(" 无监督学习")]),s._v(" "),e("p",[s._v("不同于监督学习，无监督学习的样本都是未知的，没有属性或者标签，即没有正确答案的，所有样本都是一样的，无区别的，如图所示。那么无监督学习也分为2种，一种是聚类算法，还有是鸡尾酒宴席问题。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(554),alt:"监督学习"}})]),s._v(" "),e("p",[e("img",{attrs:{src:a(555),alt:"无监督学习"}})]),s._v(" "),e("h3",{attrs:{id:"聚类算法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#聚类算法"}},[s._v("#")]),s._v(" 聚类算法")]),s._v(" "),e("p",[s._v("所谓聚类算法，就是在不知道数据集的分组情况下，对数据集进行分组，比如聚合相同的新闻，聚合相同兴趣的人（物以类聚，人以群分）。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(556),alt:"聚类算法"}})]),s._v(" "),e("h3",{attrs:{id:"鸡尾酒宴席问题（分散算法）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#鸡尾酒宴席问题（分散算法）"}},[s._v("#")]),s._v(" 鸡尾酒宴席问题（分散算法）")]),s._v(" "),e("p",[s._v("同样是对数据集进行分组，或者说是分离，这里只显示了2个输入的情况，如果输入源更多，情况会复杂很多。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(557),alt:"鸡尾酒宴席问题"}})]),s._v(" "),e("p",[s._v("下面给出这两个问题的描述。")]),s._v(" "),e("div",{staticClass:"language-sh line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[s._v("**Example:**\n\nClustering: Take a collection of "),e("span",{pre:!0,attrs:{class:"token number"}},[s._v("1,000")]),s._v(",000 different genes, and "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("find")]),s._v(" a way to automatically group these genes into "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("groups")]),s._v(" that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n\nNon-clustering: The "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Cocktail Party Algorithm"')]),s._v(", allows you to "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("find")]),s._v(" structure "),e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" a chaotic environment. "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("i.e. identifying individual voices and music from a mesh of sounds at a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br")])]),e("h2",{attrs:{id:"模型展示"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#模型展示"}},[s._v("#")]),s._v(" 模型展示")]),s._v(" "),e("p",[s._v("监督学习中的模型表示，这里用的是线性回归模型。用x表示输入数据，y表示输出数据，建立模型就是要建立x和y之间的联系，这里用h(x)来表示，因为h(x)的定义是线性的函数，所以称为线性回归模型。\n我们的目标就是建立有效的函数映射，使得，当X → Y，h(x)能很好的对y的值进行预测。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(558),alt:"线性回归模型"}})]),s._v(" "),e("p",[s._v("线性回归模型能应用于监督学习的2种类别，即回归和分类，它都非常有效。")]),s._v(" "),e("h2",{attrs:{id:"代价函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#代价函数"}},[s._v("#")]),s._v(" 代价函数")]),s._v(" "),e("p",[s._v("先给出代价函数的定义。")]),s._v(" "),e("p",[s._v("$$\nJ(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum "),e("em",[s._v("{i=1}^m \\left ( \\hat{y}")]),s._v("{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum "),e("em",[s._v("{i=1}^m \\left (h")]),s._v("\\theta (x_{i}) - y_{i} \\right)^2\n$$")]),s._v(" "),e("p",[s._v("这里的hx指的是估计函数，也就是最后要拟合的曲线，我们的目标就是要找到最适合的估计函数hx，反应到J中，就是要找到代价函数最小值对应的参数值。也就是通过代价函数来测量估计函数的拟合程度。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(559),alt:"估计函数和代价函数"}})]),s._v(" "),e("p",[s._v("代价函数在概率论中也称为均方差或者平方差，反应了数据整体的分布情况。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(560),alt:"代价函数定义"}})]),s._v(" "),e("h2",{attrs:{id:"可视化代价函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#可视化代价函数"}},[s._v("#")]),s._v(" 可视化代价函数")]),s._v(" "),e("p",[s._v("为了将代价函数可视化，这里先选择过原点的估计函数，要注意的是估计函数是关于x的函数，而代价函数是关于参数Θ的函数。那么我们可以固定Θ的值，来画出hx，再根据hx得到估计函数J的图像，如下所示：")]),s._v(" "),e("p",[e("img",{attrs:{src:a(561),alt:"代价函数"}})]),s._v(" "),e("p",[s._v("注意到，我们的目标是获得代价函数的最小值，很容易看到，当估计函数比较简单的时候，当我们选取通过所有样本的函数时，此时代价函数的值最小，为0，说明此时估计函数完美的估计了所有的样本，是最佳的情况。")]),s._v(" "),e("p",[s._v("但是上述只是最简单的情况，对于一般的情形来说，难以用二维的图像来说明。这里视频中使用了轮廓图的概念，如下图所示，通过横轴和纵轴的2个参数来整体反应代价函数的变化情况，和地理上的等高线差不多，同一条线上的代价函数值相同。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(562),alt:"二维代价函数"}})]),s._v(" "),e("p",[e("img",{attrs:{src:a(563),alt:"二维代价函数"}})]),s._v(" "),e("h2",{attrs:{id:"梯度下降算法-gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降算法-gradient-descent"}},[s._v("#")]),s._v(" 梯度下降算法 (Gradient Descent)")]),s._v(" "),e("p",[s._v("梯度下降算法，可以将代价函数J最小化，公式如下。其中的α表示学习速率，即步长，理解为每次走的距离，还要注意的是，这个公式是对Θ0和Θ1同时进行的，也就是二者应该同步，不能先算一方向，然后更新，应该同时进行。此处的偏导数对应该点在x方向和y方向分别对应的梯度，这样是下降最快的方式，用以求解到局部最优解。")]),s._v(" "),e("p",[s._v("$$\n\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\n$$")]),s._v(" "),e("p",[e("img",{attrs:{src:a(564),alt:"梯度下降算法"}})]),s._v(" "),e("p",[s._v("梯度下降算法还需要注意的一点是，该算法需要指定初始化点，对于距离很近的点来说，也可能得到不同的局部最优解（类似蝴蝶效应）如下图所示。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(565),alt:"梯度下降算法"}})]),s._v(" "),e("p",[s._v("关于参数a，需要指出的有2点。")]),s._v(" "),e("ul",[e("li",[s._v("1.α选取要适当，过小，每次移动的距离会很小，耗时较多。；过大，每次移动的距离会很大，有可能会导致不能收敛到局部最优的情况。")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(566),alt:"梯度下降算法"}})]),s._v(" "),e("ul",[e("li",[s._v("2.当α固定时，改算法能正确找到局部最优解，原因是随着斜率的缩小，在α不变的情况下，每次Θi减少的距离也缩小了，这样能保证收敛到局部最优的情况，当其已经在具备最优的情况下时，斜率为0，将不再变化，即得到结果。")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(567),alt:"梯度下降算法"}})]),s._v(" "),e("h1",{attrs:{id:"应用于线性回归的梯度下降算法-gradient-descent-for-linear-regression"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#应用于线性回归的梯度下降算法-gradient-descent-for-linear-regression"}},[s._v("#")]),s._v(" 应用于线性回归的梯度下降算法(Gradient Descent For Linear Regression)")]),s._v(" "),e("p",[s._v("那么这时候，对于线性回归模型，我们有hx和J，对于J，我们可以使用梯度下降算法来计算每一步的最优解，如下图所示。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(568),alt:"线性回归和梯度下降"}})]),s._v(" "),e("p",[s._v("那么对于J的2个参数来说，我们根据梯度下降算法，分别对J中的2个变量求J的偏导数，就能得到每个方向上的梯度下降的最优解，这样得到下面2个方程。")]),s._v(" "),e("p",[e("img",{attrs:{src:a(569),alt:"梯度下降"}})]),s._v(" "),e("p",[s._v("这样就能对两个变量进行更新，同时得到的就是局部最优解，同时根据具备最优解是全局最优解这个前提，得到全局最优解。")]),s._v(" "),e("h2",{attrs:{id:"矩阵和向量"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#矩阵和向量"}},[s._v("#")]),s._v(" 矩阵和向量")]),s._v(" "),e("p",[s._v("这里都是现代里面的知识了，我就不多说了，基本都会。")]),s._v(" "),e("p",[e("strong",[s._v("Week 1 ends here.")])])])}),[],!1,null,null,null);t.default=n.exports}}]);