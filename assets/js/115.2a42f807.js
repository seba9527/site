(window.webpackJsonp=window.webpackJsonp||[]).push([[115],{556:function(t,a,_){"use strict";_.r(a);var e=_(14),s=Object(e.a)({},(function(){var t=this,a=t.$createElement,_=t._self._c||a;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("p",[t._v("机器学习笔记week2。")]),t._v(" "),_("hr"),t._v(" "),_("h2",{attrs:{id:"多变量线性回归"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#多变量线性回归"}},[t._v("#")]),t._v(" 多变量线性回归")]),t._v(" "),_("p",[t._v("假设现在有n个变量影响最终的结果，我们可以这样设立方程，注意变量均是线性的。其中θ是参量，而x是变量。")]),t._v(" "),_("p",[t._v("$$\nh_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n\n$$")]),t._v(" "),_("p",[t._v("我们也可以将上述表达式写成矩阵相乘的形式，如下图所示。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_16-15-46.png",alt:"多变量线性回归"}})]),t._v(" "),_("p",[t._v("即\n$$\nh_\\theta (x) = \\theta^Tx\n$$")]),t._v(" "),_("h2",{attrs:{id:"多变量代价函数和梯度函数"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#多变量代价函数和梯度函数"}},[t._v("#")]),t._v(" 多变量代价函数和梯度函数")]),t._v(" "),_("p",[t._v("然后我们就能得到估计函数，参数，代价函数的表达式，如下图所示，并且我们将关于θ的参数看成一个整体，作为θ的行向量，然后我们定义梯度下降函数的表达式，即对代价函数中每个变量求偏导数。\n"),_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_16-46-44.png",alt:"多变量代价函数"}})]),t._v(" "),_("p",[t._v("如下图所示，我们可以对比单变量和多变量的情况，其实类似，由于我们定义了初始值是1，所以所有变量的情况和单变量情况下保持一致。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_16-43-56.png",alt:"多变量梯度下降"}})]),t._v(" "),_("p",[t._v("公式如下所示。")]),t._v(" "),_("p",[t._v("$$\nθ_j:=θ_j−α\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})⋅x^{(i)}_j\n$$")]),t._v(" "),_("h2",{attrs:{id:"特征缩放"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#特征缩放"}},[t._v("#")]),t._v(" 特征缩放")]),t._v(" "),_("p",[t._v("进行特征缩放的意义就是在于，如果多变量之间的差距较大，在进行梯度下降的过程中，我们进行梯度下降的时间会比较慢，次数也比较多，如下图所示。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_17-20-56.png",alt:"特征缩放"}})]),t._v(" "),_("p",[t._v("我们将每个变量进行缩放，将其缩放到区间-1到1之间，是比较好的情况。")]),t._v(" "),_("h3",{attrs:{id:"均值归一化"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#均值归一化"}},[t._v("#")]),t._v(" 均值归一化")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_17-23-19.png",alt:"均值归一化"}})]),t._v(" "),_("p",[t._v("如上图所示，我们可以使用变量进行替换。具体方式减去变量的平均值再除以范围（标准差），就能得到一个很好的范围来进行梯度下降。")]),t._v(" "),_("h2",{attrs:{id:"关于学习比例α"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#关于学习比例α"}},[t._v("#")]),t._v(" 关于学习比例α")]),t._v(" "),_("p",[t._v("如何确定α的值是比较困难的，这里视频中提出了一种方法，即画出每一次迭代和代价函数的相应值，用以检测每一次迭代是否正确。如下图所示。同时我们可以选定一个极小值，如果在一次迭代中代价函数的值减小的部分小于极小值，可以认为代价函数已经根据梯度下降算法找到了最小值。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_17-46-11.png",alt:"代价函数和迭代次数"}})]),t._v(" "),_("p",[t._v("如果代价函数随着迭代次数不是逐渐下降的，那我们可以认为α的值设定错误，大多数情况下都是偏大了，如下图所示，这些情况的原因都是α偏大，所以画出图像是一个很好的检测方式来判断α的值是否恰当。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_17-47-16.png",alt:"学习比例α"}})]),t._v(" "),_("h2",{attrs:{id:"多项式线性回归"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#多项式线性回归"}},[t._v("#")]),t._v(" 多项式线性回归")]),t._v(" "),_("p",[t._v("讨论如何拟合多项式，一般的方式是通过换元法，即将变量替代成高次，同时注意控制变量的范围，这里需要和之前特征缩放结合起来，使得变量之间差距不要过大。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_19-06-32.png",alt:"多项式线性回归"}})]),t._v(" "),_("h2",{attrs:{id:"正规方程"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#正规方程"}},[t._v("#")]),t._v(" 正规方程")]),t._v(" "),_("p",[t._v("正规方程能给出一个标准解法来求解代价函数的最小值，图解如下所示。当你有m个数据，每个数据n个特征值时，根据如下的解法，我们能得到最小值。")]),t._v(" "),_("p",[_("img",{attrs:{src:"/img/2018-12/Snipaste_2018-12-22_19-37-23.png",alt:"正规方程"}})]),t._v(" "),_("p",[t._v("即")]),t._v(" "),_("p",[t._v("$$\n\\theta=(X^TX)^{-1}X^Ty\n$$")]),t._v(" "),_("h3",{attrs:{id:"梯度下降和正规方程的比较"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降和正规方程的比较"}},[t._v("#")]),t._v(" 梯度下降和正规方程的比较")]),t._v(" "),_("table",[_("thead",[_("tr",[_("th",{staticStyle:{"text-align":"center"}},[t._v("梯度下降")]),t._v(" "),_("th",{staticStyle:{"text-align":"center"}},[t._v("正规方程")])])]),t._v(" "),_("tbody",[_("tr",[_("td",{staticStyle:{"text-align":"center"}},[t._v("需要选择α")]),t._v(" "),_("td",{staticStyle:{"text-align":"center"}},[t._v("不需要α")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"center"}},[t._v("需要很多次迭代")]),t._v(" "),_("td",{staticStyle:{"text-align":"center"}},[t._v("不需要迭代")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"center"}},[t._v("运算复杂度是n的平方")]),t._v(" "),_("td",{staticStyle:{"text-align":"center"}},[t._v("运算复杂度是n的立方，且需要计算逆矩阵")])]),t._v(" "),_("tr",[_("td",{staticStyle:{"text-align":"center"}},[t._v("当n很大时，仍然有效")]),t._v(" "),_("td",{staticStyle:{"text-align":"center"}},[t._v("当n很大时，速度很慢")])])])]),t._v(" "),_("h3",{attrs:{id:"关于矩阵不可逆"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#关于矩阵不可逆"}},[t._v("#")]),t._v(" 关于矩阵不可逆")]),t._v(" "),_("p",[t._v("如果碰到矩阵不可逆的情况，一般有2种方式来解决这个问题，首先是检测是否存在多余的特征变量，即多个变量之间线性相关，这会导致这个问题。其次检测是否特征变量太多，可以选择删除某些变量，或者使用正规化来解决这个问题。")]),t._v(" "),_("p",[_("strong",[t._v("Week 2 ends here.")])])])}),[],!1,null,null,null);a.default=s.exports}}]);